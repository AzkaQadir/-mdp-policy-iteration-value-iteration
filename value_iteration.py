# -*- coding: utf-8 -*-
"""value_iteration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBZVgpFnHtmFbEo6CIpziF2X6sj7ZDIp
"""

import numpy as np

def value_iteration(states, actions, transitions, gamma=0.9, theta=1e-6):
    V = {s: 0 for s in states}  # Initialize values to 0
    policy = {s: None for s in states}  # Initialize policy
    iteration = 0

    while True:
        delta = 0
        new_V = V.copy()
        for s in states:
            max_value = float('-inf')
            best_action = None

            for a in actions:
                value = sum(p * (r + gamma * V[s_next]) for (p, s_next, r) in transitions[s][a])
                if value > max_value:
                    max_value = value
                    best_action = a

            new_V[s] = max_value
            policy[s] = best_action
            delta = max(delta, abs(V[s] - new_V[s]))

        V = new_V
        iteration += 1
        if delta < theta:
            break

    return V, policy, iteration

# Define states
states = ['Top', 'Rolling', 'Bottom']

# Define actions
actions = ['Drive', 'Don’t Drive']

# Define transition probabilities and rewards
transitions = {
    'Top': {
        'Drive': [(0.5, 'Top', 2), (0.5, 'Rolling', 2)],
        'Don’t Drive': [(0.5, 'Top', 3), (0.5, 'Rolling', 1)],
    },
    'Rolling': {
        'Drive': [(0.3, 'Top', 2), (0.4, 'Rolling', 1.5), (0.3, 'Bottom', 0.5)],
        'Don’t Drive': [(1.0, 'Bottom', 1)],
    },
    'Bottom': {
        'Drive': [(0.5, 'Top', 2), (0.5, 'Bottom', 2)],
        'Don’t Drive': [(1.0, 'Bottom', 1)],
    },
}

# Run value iteration
optimal_values, optimal_policy, iterations = value_iteration(states, actions, transitions)

# Display results
print("Optimal Values:")
for state in states:
    print(f"{state}: {optimal_values[state]:.4f}")

print("\nOptimal Policy:")
for state in states:
    print(f"{state}: {optimal_policy[state]}")

print(f"\nConverged in {iterations} iterations")

