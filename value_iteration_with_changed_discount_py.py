# -*- coding: utf-8 -*-
"""value_iteration_with_changed_discount.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bIzM3Egnai-SNroscxKdUs1DuqONMX7B
"""

import numpy as np

# Define states and actions
states = ["Top", "Rolling", "Bottom"]
actions = ["Drive", "Don't Drive"]

# Define transition probabilities and rewards
transitions = {
    "Top": {
        "Drive": [(0.5, "Top", 2), (0.5, "Rolling", 2)],
        "Don't Drive": [(0.5, "Top", 3), (0.5, "Rolling", 1)]
    },
    "Rolling": {
        "Drive": [(0.3, "Top", 2), (0.4, "Rolling", 1.5), (0.3, "Bottom", 0.5)],
        "Don't Drive": [(1.0, "Bottom", 1)]
    },
    "Bottom": {
        "Drive": [(0.5, "Top", 2), (0.5, "Bottom", 2)],
        "Don't Drive": [(1.0, "Bottom", 1)]
    }
}

def value_iteration(states, actions, transitions, gamma=0.75, theta=1e-6):
    values = {s: 0 for s in states}
    policy = {s: None for s in states}
    iterations = 0

    while True:
        delta = 0
        new_values = values.copy()

        for state in states:
            action_values = {}
            for action in actions:
                action_values[action] = sum(prob * (reward + gamma * values[next_state])
                                            for prob, next_state, reward in transitions[state][action])

            best_action = max(action_values, key=action_values.get)
            new_values[state] = action_values[best_action]
            policy[state] = best_action
            delta = max(delta, abs(new_values[state] - values[state]))

        values = new_values
        iterations += 1
        if delta < theta:
            break

    return values, policy, iterations

optimal_values, optimal_policy, iterations = value_iteration(states, actions, transitions)

print("Optimal Values with Changed Discount Factor:", optimal_values)
print("Optimal Policy with Changed Discount Factor:", optimal_policy)
print("Number of Iterations:", iterations)