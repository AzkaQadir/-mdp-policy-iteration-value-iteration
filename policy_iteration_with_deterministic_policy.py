# -*- coding: utf-8 -*-
"""policy_iteration_with_deterministic_policy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xtH44fgQFaWlyG4cee4CvUuUc1bZNmxE
"""

import numpy as np

def policy_evaluation(policy, states, actions, transitions, gamma=0.9, theta=1e-6):
    V = {s: 0 for s in states}  # Initialize value function
    while True:
        delta = 0
        new_V = V.copy()
        for s in states:
            value = 0
            for a, action_prob in policy[s].items():
                for p, s_next, r in transitions[s][a]:
                    value += action_prob * p * (r + gamma * V[s_next])
            new_V[s] = value
            delta = max(delta, abs(V[s] - new_V[s]))
        V = new_V
        if delta < theta:
            break
    return V

def policy_improvement(states, actions, transitions, V, gamma=0.9):
    policy = {s: {a: 0 for a in actions} for s in states}
    for s in states:
        action_values = {}
        for a in actions:
            action_values[a] = sum(p * (r + gamma * V[s_next]) for p, s_next, r in transitions[s][a])
        best_action = max(action_values, key=action_values.get)
        policy[s] = {a: 1 if a == best_action else 0 for a in actions}  # Deterministic policy
    return policy

def policy_iteration(states, actions, transitions, initial_policy, gamma=0.9):
    policy = initial_policy.copy()
    iterations = 0
    while True:
        V = policy_evaluation(policy, states, actions, transitions, gamma)
        new_policy = policy_improvement(states, actions, transitions, V, gamma)
        iterations += 1
        if new_policy == policy:
            break
        policy = new_policy
    return V, policy, iterations

# Define states and actions
states = ['Top', 'Rolling', 'Bottom']
actions = ['Drive', 'Don’t Drive']

# Define transition probabilities and rewards
transitions = {
    'Top': {
        'Drive': [(0.5, 'Top', 2), (0.5, 'Rolling', 2)],
        'Don’t Drive': [(0.5, 'Top', 3), (0.5, 'Rolling', 1)],
    },
    'Rolling': {
        'Drive': [(0.3, 'Top', 2), (0.4, 'Rolling', 1.5), (0.3, 'Bottom', 0.5)],
        'Don’t Drive': [(1.0, 'Bottom', 1)],
    },
    'Bottom': {
        'Drive': [(0.5, 'Top', 2), (0.5, 'Bottom', 2)],
        'Don’t Drive': [(1.0, 'Bottom', 1)],
    },
}

# Deterministic initial policy (Never Drives)
deterministic_policy = {s: {'Drive': 0, 'Don’t Drive': 1} for s in states}
optimal_values_deterministic, optimal_policy_deterministic, iterations_deterministic = policy_iteration(states, actions, transitions, deterministic_policy)

print("Optimal Values (Deterministic Start - Never Drives):")
for state in states:
    print(f"{state}: {optimal_values_deterministic[state]:.4f}")
print("\nOptimal Policy (Deterministic Start - Never Drives):")
for state in states:
    print(f"{state}: {optimal_policy_deterministic[state]}")
print(f"\nConverged in {iterations_deterministic} iterations")